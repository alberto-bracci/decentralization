{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae1bb710-41ae-4a56-8b1d-e7e9e96fb990",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9b35632-e6db-4b74-862f-2ad2d28d8cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef38be8-7971-44ce-bc0d-c9726d11e534",
   "metadata": {},
   "source": [
    "# Collecting data from Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6496f64-f27b-4620-a443-8fda78562c19",
   "metadata": {},
   "source": [
    "Download corpus from\n",
    "https://api.semanticscholar.org/corpus/download/\n",
    "\n",
    "Release downloaded: 2021-09-01 release\n",
    "\n",
    "From the root folder, run the following commands:\n",
    "\n",
    "```\n",
    "mkdir corpus\n",
    "cd corpus\n",
    "wget https://s3-us-west-2.amazonaws.com/ai2-s2-research-public/open-corpus/2021-09-01/manifest.txt\n",
    "wget -B https://s3-us-west-2.amazonaws.com/ai2-s2-research-public/open-corpus/2021-09-01/ -i manifest.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5086154-c9da-4959-8311-5de0318d34f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to repo root folder\n",
    "# ACHTUNG: do only once!! \n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cc6d164-6014-4283-adf4-3ed6e64fd45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_version = '2021-09-01'\n",
    "corpus_folder = os.path.join('corpus', corpus_version) # where you have the corpus\n",
    "data_folder = os.path.join('data', corpus_version) # where you save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f968bca-c21d-4fe4-9cc0-155175bff763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose keyword and the words you want to check to be inside the title or abstract\n",
    "words_to_check_dict = {\n",
    "    'decentralization':['centraliz','centralis'],\n",
    "    'internet': ['internet'],\n",
    "    'wireless': ['wireless'],\n",
    "    'social media': ['social media', 'social network'],\n",
    "    'hiv': ['hiv'],\n",
    "    'covid': ['covid', 'coronavirus'],\n",
    "    'dark web': ['deep web', 'dark web']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9f9d16-8955-49fd-bb35-1958d670d2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3fd8ad98364f888877484d315d9fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# select the papers with the chosen keywords\n",
    "all_docs_dict = {key:{} for key in words_to_check_dict}\n",
    "# also save some statistics on the whole dataset\n",
    "no_papers_in_fields_by_year = {}\n",
    "sets_authors_in_fields_by_year = {}\n",
    "sets_authors_by_year = {}\n",
    "no_authors_in_fields_by_year = {}\n",
    "no_authors_by_year = {}\n",
    "\n",
    "count = 0\n",
    "start = datetime.now()\n",
    "\n",
    "for ID in tqdm(range(6000)):\n",
    "    filename = os.path.join(corpus_folder,'s2-corpus-%.3d.gz'%ID)\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        for item in jsonlines.Reader(f):\n",
    "            # each line of the file is a dictionary with the paper's info\n",
    "            x=item\n",
    "            count += 1\n",
    "            title = x['title'].lower()\n",
    "            abstract = x['paperAbstract'].lower()\n",
    "            \n",
    "            for keyword,words_to_check in words_to_check_dict.items():\n",
    "                # check if this paper has the words to check\n",
    "                add_it = False\n",
    "                for word in words_to_check:\n",
    "                    if word in title or word in abstract: # if you want to check only in title/abstract, change it here\n",
    "                        add_it = True\n",
    "                        break\n",
    "                if add_it:\n",
    "                    all_docs_dict[keyword][x['id']] = x.copy()\n",
    "                \n",
    "            # get statistics of whole dataset\n",
    "            year = x['year']\n",
    "            if year not in no_papers_in_fields_by_year:\n",
    "                no_papers_in_fields_by_year[year] = {}\n",
    "                sets_authors_in_fields_by_year[year] = {}\n",
    "                sets_authors_by_year[year] = set()\n",
    "            year_dict = no_papers_in_fields_by_year[year]\n",
    "            fields = tuple(x['fieldsOfStudy'])\n",
    "            if fields not in year_dict:\n",
    "                year_dict[fields] = 1\n",
    "                sets_authors_in_fields_by_year[year][fields] = set()\n",
    "            else:\n",
    "                year_dict[fields] += 1\n",
    "            for authors in x['authors']:\n",
    "                sets_authors_in_fields_by_year[year][fields].update(authors['ids'])\n",
    "                sets_authors_by_year[year].update(authors['ids'])\n",
    "                \n",
    "    end = datetime.now()\n",
    "\n",
    "    if (ID+1) % 1000 == 0:\n",
    "        print(f'Read {ID+1} files after {end-start}.',flush=True)\n",
    "        for keyword,papers in all_docs_dict.items():\n",
    "              print(f'\\tkeyword: {keyword} - no.papers: {len(papers)}')\n",
    "\n",
    "for keyword in words_to_check_dict:\n",
    "    keyword_data_folder = os.path.join(data_folder, keyword)\n",
    "    os.makedirs(keyword_data_folder,exist_ok=True)\n",
    "    with gzip.open(os.path.join(keyword_data_folder, 'papers_dict.pkl.gz'), 'wb') as fp:\n",
    "        pickle.dump(all_docs_dict[keyword],fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af6ec65-e9e9-461b-8d46-80c072ded107",
   "metadata": {},
   "source": [
    "Counting how many papers among the selected ones have each (important) attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30be874d-9112-4f81-adf4-d4fef1369b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for keyword,papers in all_docs_dict.items():\n",
    "    print(f'keyword: {keyword}\\n\\nno.papers: {len(papers)}')\n",
    "    \n",
    "    no_doi = []\n",
    "    count_doi = 0\n",
    "    for paper, paper_dict in papers.items():\n",
    "        if paper_dict['doi'] is None or len(paper_dict['doi']) == 0:\n",
    "            no_doi.append(paper)\n",
    "        else:\n",
    "            count_doi += 1\n",
    "    print(f'{count_doi} have doi of total {len(papers)}')\n",
    "\n",
    "    no_year = []\n",
    "    count_year = 0\n",
    "    for paper, paper_dict in papers.items():\n",
    "        if paper_dict['year'] is None:\n",
    "            no_year.append(paper)\n",
    "        else:\n",
    "            count_year += 1\n",
    "    print(f'{count_year} have year of total {len(papers)}')\n",
    "\n",
    "    no_fields = []\n",
    "    count_fields = 0\n",
    "    for paper, paper_dict in papers.items():\n",
    "        if paper_dict['fieldsOfStudy'] is None or len(paper_dict['fieldsOfStudy']) == 0:\n",
    "            no_year.append(paper)\n",
    "        else:\n",
    "            count_fields += 1\n",
    "    print(f'{count_fields} have fields of total {len(papers)}')\n",
    "\n",
    "    no_good = []\n",
    "    count_good = 0\n",
    "    for paper, paper_dict in papers.items():\n",
    "        if paper_dict['doi'] is None or len(paper_dict['doi']) == 0 or paper_dict['year'] is None or paper_dict['fieldsOfStudy'] is None or len(paper_dict['fieldsOfStudy']) == 0 or paper_dict['paperAbstract'] is None or len(paper_dict['paperAbstract']) == 0 or paper_dict['title'] is None or len(paper_dict['title']) == 0 or ( len(paper_dict['inCitations']) == 0 and len(paper_dict['outCitations']) == 0 ):\n",
    "            no_good.append(paper)\n",
    "        else:\n",
    "            count_good += 1\n",
    "    print(f'{count_good} are good of total {len(papers)} (have doi, year, fields, abstract, title, and at least one of reference or citation)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf8033f-86b1-42ca-892a-6372a9cb0d9c",
   "metadata": {},
   "source": [
    "Save dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8017cdf4-c38d-451d-9304-45a1ae8b558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(os.path.join(data_folder, 'no_papers_in_fields_by_year.pkl.gz'), 'wb') as fp:\n",
    "    pickle.dump(no_papers_in_fields_by_year,fp)\n",
    "\n",
    "with gzip.open(os.path.join(data_folder, 'sets_authors_in_fields_by_year.pkl.gz'), 'wb') as fp:\n",
    "    pickle.dump(sets_authors_in_fields_by_year,fp)\n",
    "\n",
    "for year, year_dict in sets_authors_in_fields_by_year.items():\n",
    "    no_authors_in_fields_by_year[year] = {}\n",
    "    for fields, set_authors in year_dict.items():\n",
    "        no_authors_in_fields_by_year[year][fields] = len(set_authors)\n",
    "with gzip.open(os.path.join(data_folder, 'no_authors_in_fields_by_year.pkl.gz'), 'wb') as fp:\n",
    "    pickle.dump(no_authors_in_fields_by_year,fp)\n",
    "\n",
    "for year, year_dict in sets_authors_by_year.items():\n",
    "    no_authors_by_year[year] = len(year_dict)\n",
    "with gzip.open(os.path.join(data_folder, 'no_authors_by_year.pkl.gz'), 'wb') as fp:\n",
    "    pickle.dump(no_authors_by_year,fp)\n",
    "print('Everything dumped successfully!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
