{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae1bb710-41ae-4a56-8b1d-e7e9e96fb990",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9b35632-e6db-4b74-862f-2ad2d28d8cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef38be8-7971-44ce-bc0d-c9726d11e534",
   "metadata": {},
   "source": [
    "# Collecting data from Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6496f64-f27b-4620-a443-8fda78562c19",
   "metadata": {},
   "source": [
    "Download corpus from\n",
    "https://api.semanticscholar.org/corpus/download/\n",
    "\n",
    "Release downloaded: 2021-09-01 release\n",
    "\n",
    "From the root folder, run the following commands:\n",
    "\n",
    "```\n",
    "mkdir corpus\n",
    "cd corpus\n",
    "wget https://s3-us-west-2.amazonaws.com/ai2-s2-research-public/open-corpus/2021-09-01/manifest.txt\n",
    "wget -B https://s3-us-west-2.amazonaws.com/ai2-s2-research-public/open-corpus/2021-09-01/ -i manifest.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5086154-c9da-4959-8311-5de0318d34f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to repo root folder\n",
    "# ACHTUNG: do only once!! \n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc6d164-6014-4283-adf4-3ed6e64fd45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_version = '2021-09-01'\n",
    "corpus_folder = os.path.join('corpus', corpus_version) # where you have the corpus\n",
    "data_folder = os.path.join('data', corpus_version) # where you save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f968bca-c21d-4fe4-9cc0-155175bff763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose keyword and the words you want to check to be inside the title or abstract\n",
    "words_to_check_dict = {\n",
    "    'decentralization':['centraliz','centralis'],\n",
    "    'internet': ['internet'],\n",
    "    'wireless': ['wireless'],\n",
    "    'social media': ['social media', 'social network'],\n",
    "    'hiv': ['hiv'],\n",
    "    'covid': ['covid', 'coronavirus'],\n",
    "    'dark web': ['deep web', 'dark web']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe9f9d16-8955-49fd-bb35-1958d670d2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819fb67f13ff4c6eb41a8d6c775201a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1000 files after 0:42:41.777740.\n",
      "\tkeyword: decentralization - no.papers: 68095\n",
      "\tkeyword: internet - no.papers: 184577\n",
      "\tkeyword: wireless - no.papers: 120361\n",
      "\tkeyword: social media - no.papers: 65607\n",
      "\tkeyword: hiv - no.papers: 206670\n",
      "\tkeyword: covid - no.papers: 69903\n",
      "\tkeyword: dark web - no.papers: 420\n",
      "Read 2000 files after 1:26:39.296197.\n",
      "\tkeyword: decentralization - no.papers: 136081\n",
      "\tkeyword: internet - no.papers: 369254\n",
      "\tkeyword: wireless - no.papers: 240487\n",
      "\tkeyword: social media - no.papers: 131313\n",
      "\tkeyword: hiv - no.papers: 412043\n",
      "\tkeyword: covid - no.papers: 140162\n",
      "\tkeyword: dark web - no.papers: 872\n",
      "Read 3000 files after 2:11:41.723527.\n",
      "\tkeyword: decentralization - no.papers: 204467\n",
      "\tkeyword: internet - no.papers: 553871\n",
      "\tkeyword: wireless - no.papers: 360596\n",
      "\tkeyword: social media - no.papers: 197391\n",
      "\tkeyword: hiv - no.papers: 616948\n",
      "\tkeyword: covid - no.papers: 209546\n",
      "\tkeyword: dark web - no.papers: 1356\n",
      "Read 4000 files after 2:57:37.590076.\n",
      "\tkeyword: decentralization - no.papers: 272297\n",
      "\tkeyword: internet - no.papers: 738112\n",
      "\tkeyword: wireless - no.papers: 480752\n",
      "\tkeyword: social media - no.papers: 263453\n",
      "\tkeyword: hiv - no.papers: 822480\n",
      "\tkeyword: covid - no.papers: 280868\n",
      "\tkeyword: dark web - no.papers: 1775\n",
      "Read 5000 files after 3:42:15.368715.\n",
      "\tkeyword: decentralization - no.papers: 339971\n",
      "\tkeyword: internet - no.papers: 922358\n",
      "\tkeyword: wireless - no.papers: 600534\n",
      "\tkeyword: social media - no.papers: 328401\n",
      "\tkeyword: hiv - no.papers: 1028868\n",
      "\tkeyword: covid - no.papers: 352065\n",
      "\tkeyword: dark web - no.papers: 2218\n",
      "Read 6000 files after 4:25:39.584187.\n",
      "\tkeyword: decentralization - no.papers: 408231\n",
      "\tkeyword: internet - no.papers: 1106677\n",
      "\tkeyword: wireless - no.papers: 721127\n",
      "\tkeyword: social media - no.papers: 393772\n",
      "\tkeyword: hiv - no.papers: 1234409\n",
      "\tkeyword: covid - no.papers: 419402\n",
      "\tkeyword: dark web - no.papers: 2639\n"
     ]
    }
   ],
   "source": [
    "# select the papers with the chosen keywords\n",
    "all_docs_dict = {key:{} for key in words_to_check_dict}\n",
    "# also save some statistics on the whole dataset\n",
    "no_papers_in_fields_by_year = {}\n",
    "sets_authors_in_fields_by_year = {}\n",
    "sets_authors_by_year = {}\n",
    "no_authors_in_fields_by_year = {}\n",
    "no_authors_by_year = {}\n",
    "\n",
    "count = 0\n",
    "start = datetime.now()\n",
    "\n",
    "for ID in tqdm(range(6000)):\n",
    "    filename = os.path.join(corpus_folder,'s2-corpus-%.3d.gz'%ID)\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        for item in jsonlines.Reader(f):\n",
    "            # each line of the file is a dictionary with the paper's info\n",
    "            x=item\n",
    "            count += 1\n",
    "            title = x['title'].lower()\n",
    "            abstract = x['paperAbstract'].lower()\n",
    "            \n",
    "            for keyword,words_to_check in words_to_check_dict.items():\n",
    "                # check if this paper has the words to check\n",
    "                add_it = False\n",
    "                for word in words_to_check:\n",
    "                    if word in title or word in abstract: # if you want to check only in title/abstract, change it here\n",
    "                        add_it = True\n",
    "                        break\n",
    "                if add_it:\n",
    "                    all_docs_dict[keyword][x['id']] = x.copy()\n",
    "                \n",
    "            # get statistics of whole dataset\n",
    "            year = x['year']\n",
    "            if year not in no_papers_in_fields_by_year:\n",
    "                no_papers_in_fields_by_year[year] = {}\n",
    "                sets_authors_in_fields_by_year[year] = {}\n",
    "                sets_authors_by_year[year] = set()\n",
    "            year_dict = no_papers_in_fields_by_year[year]\n",
    "            fields = tuple(x['fieldsOfStudy'])\n",
    "            if fields not in year_dict:\n",
    "                year_dict[fields] = 1\n",
    "                sets_authors_in_fields_by_year[year][fields] = set()\n",
    "            else:\n",
    "                year_dict[fields] += 1\n",
    "            for authors in x['authors']:\n",
    "                sets_authors_in_fields_by_year[year][fields].update(authors['ids'])\n",
    "                sets_authors_by_year[year].update(authors['ids'])\n",
    "                \n",
    "    end = datetime.now()\n",
    "\n",
    "    if (ID+1) % 1000 == 0:\n",
    "        print(f'Read {ID+1} files after {end-start}.',flush=True)\n",
    "        for keyword,papers in all_docs_dict.items():\n",
    "              print(f'\\tkeyword: {keyword} - no.papers: {len(papers)}')\n",
    "\n",
    "for keyword in words_to_check_dict:\n",
    "    keyword_data_folder = os.path.join(data_folder, keyword)\n",
    "    os.makedirs(keyword_data_folder,exist_ok=True)\n",
    "    with gzip.open(os.path.join(keyword_data_folder, 'papers_dict.pkl.gz'), 'wb') as fp:\n",
    "        pickle.dump(all_docs_dict[keyword],fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af6ec65-e9e9-461b-8d46-80c072ded107",
   "metadata": {},
   "source": [
    "Counting how many papers among the selected ones have each (important) attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30be874d-9112-4f81-adf4-d4fef1369b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyword: decentralization\n",
      "\n",
      "no.papers: 408231\n",
      "236927 have doi\n",
      "406816 have year\n",
      "369744 have fields\n",
      "294720 have at least one reference or citation\n",
      "194851 have everything (have doi, year, fields, abstract, title, and at least one reference or citation)\n",
      "\n",
      "\n",
      "\n",
      "keyword: internet\n",
      "\n",
      "no.papers: 1106677\n",
      "563575 have doi\n",
      "1102442 have year\n",
      "1006002 have fields\n",
      "709246 have at least one reference or citation\n",
      "435890 have everything (have doi, year, fields, abstract, title, and at least one reference or citation)\n",
      "\n",
      "\n",
      "\n",
      "keyword: wireless\n",
      "\n",
      "no.papers: 721127\n",
      "433089 have doi\n",
      "718007 have year\n",
      "659158 have fields\n",
      "545205 have at least one reference or citation\n",
      "386614 have everything (have doi, year, fields, abstract, title, and at least one reference or citation)\n",
      "\n",
      "\n",
      "\n",
      "keyword: social media\n",
      "\n",
      "no.papers: 393772\n",
      "251130 have doi\n",
      "392958 have year\n",
      "363665 have fields\n",
      "300067 have at least one reference or citation\n",
      "200122 have everything (have doi, year, fields, abstract, title, and at least one reference or citation)\n",
      "\n",
      "\n",
      "\n",
      "keyword: hiv\n",
      "\n",
      "no.papers: 1234409\n",
      "749703 have doi\n",
      "1230692 have year\n",
      "1117544 have fields\n",
      "806160 have at least one reference or citation\n",
      "553710 have everything (have doi, year, fields, abstract, title, and at least one reference or citation)\n",
      "\n",
      "\n",
      "\n",
      "keyword: covid\n",
      "\n",
      "no.papers: 419402\n",
      "354992 have doi\n",
      "419001 have year\n",
      "376709 have fields\n",
      "282567 have at least one reference or citation\n",
      "222770 have everything (have doi, year, fields, abstract, title, and at least one reference or citation)\n",
      "\n",
      "\n",
      "\n",
      "keyword: dark web\n",
      "\n",
      "no.papers: 2639\n",
      "1479 have doi\n",
      "2626 have year\n",
      "2330 have fields\n",
      "1953 have at least one reference or citation\n",
      "1215 have everything (have doi, year, fields, abstract, title, and at least one reference or citation)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for keyword,papers in all_docs_dict.items():\n",
    "    print(f'keyword: {keyword}\\n\\nno.papers: {len(papers)}')\n",
    "    \n",
    "    no_doi = []\n",
    "    count_doi = 0\n",
    "    for paper, paper_dict in papers.items():\n",
    "        if paper_dict['doi'] is None or len(paper_dict['doi']) == 0:\n",
    "            no_doi.append(paper)\n",
    "        else:\n",
    "            count_doi += 1\n",
    "    print(f'{count_doi} have doi')\n",
    "\n",
    "    no_year = []\n",
    "    count_year = 0\n",
    "    for paper, paper_dict in papers.items():\n",
    "        if paper_dict['year'] is None:\n",
    "            no_year.append(paper)\n",
    "        else:\n",
    "            count_year += 1\n",
    "    print(f'{count_year} have year')\n",
    "\n",
    "    no_fields = []\n",
    "    count_fields = 0\n",
    "    for paper, paper_dict in papers.items():\n",
    "        if paper_dict['fieldsOfStudy'] is None or len(paper_dict['fieldsOfStudy']) == 0:\n",
    "            no_year.append(paper)\n",
    "        else:\n",
    "            count_fields += 1\n",
    "    print(f'{count_fields} have fields')\n",
    "    \n",
    "    no_cit = []\n",
    "    count_cit = 0\n",
    "    for paper, paper_dict in papers.items():\n",
    "        if ( len(paper_dict['inCitations']) == 0 and len(paper_dict['outCitations']) == 0 ):\n",
    "            no_cit.append(paper)\n",
    "        else:\n",
    "            count_cit += 1\n",
    "    print(f'{count_cit} have at least one reference or citation')\n",
    "    \n",
    "    no_good = []\n",
    "    count_good = 0\n",
    "    for paper, paper_dict in papers.items():\n",
    "        if paper_dict['doi'] is None or len(paper_dict['doi']) == 0 or paper_dict['year'] is None or paper_dict['fieldsOfStudy'] is None or len(paper_dict['fieldsOfStudy']) == 0 or paper_dict['paperAbstract'] is None or len(paper_dict['paperAbstract']) == 0 or paper_dict['title'] is None or len(paper_dict['title']) == 0 or ( len(paper_dict['inCitations']) == 0 and len(paper_dict['outCitations']) == 0 ):\n",
    "            no_good.append(paper)\n",
    "        else:\n",
    "            count_good += 1\n",
    "    print(f'{count_good} have everything (have doi, year, fields, abstract, title, and at least one reference or citation)')\n",
    "    \n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf8033f-86b1-42ca-892a-6372a9cb0d9c",
   "metadata": {},
   "source": [
    "Saving now dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8017cdf4-c38d-451d-9304-45a1ae8b558f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything dumped successfully!\n"
     ]
    }
   ],
   "source": [
    "with gzip.open(os.path.join(data_folder, 'no_papers_in_fields_by_year.pkl.gz'), 'wb') as fp:\n",
    "    pickle.dump(no_papers_in_fields_by_year,fp)\n",
    "\n",
    "with gzip.open(os.path.join(data_folder, 'sets_authors_in_fields_by_year.pkl.gz'), 'wb') as fp:\n",
    "    pickle.dump(sets_authors_in_fields_by_year,fp)\n",
    "\n",
    "for year, year_dict in sets_authors_in_fields_by_year.items():\n",
    "    no_authors_in_fields_by_year[year] = {}\n",
    "    for fields, set_authors in year_dict.items():\n",
    "        no_authors_in_fields_by_year[year][fields] = len(set_authors)\n",
    "with gzip.open(os.path.join(data_folder, 'no_authors_in_fields_by_year.pkl.gz'), 'wb') as fp:\n",
    "    pickle.dump(no_authors_in_fields_by_year,fp)\n",
    "\n",
    "for year, year_dict in sets_authors_by_year.items():\n",
    "    no_authors_by_year[year] = len(year_dict)\n",
    "with gzip.open(os.path.join(data_folder, 'no_authors_by_year.pkl.gz'), 'wb') as fp:\n",
    "    pickle.dump(no_authors_by_year,fp)\n",
    "print('Everything dumped successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9426cff9-a737-45ec-a714-427f04a15a84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
